{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Доклад по Scrapy \n",
    "Выполнил Хоранян Нарек ЭАД"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy? Что это вообще?\n",
    "\n",
    "Процитируем документацию:\n",
    ">Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n",
    "\n",
    ">Even though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переведем с технического на человческий:\n",
    "Scrapy - это ФРЕЙМВОРК, который позволяют ползти по сайтам и извлекать структурированные данные типа таблиц и вообще чего угодно что написано нормальным хтмлевым языком, то есть мега удобный парсер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Почему Scrapy а не пацанские самопалы через request/aiohttp?\n",
    "1. В основе Scrapy лежит *асинхронный* движок\n",
    "2. Поддержка почти чего угодно:\n",
    "   1. CSS-селекторы\n",
    "   2. Генерация экспортов в множестве формаотв (JSON, CSV, XML) и их хранение в разных бэкендах (FTP, S3, локально)\n",
    "   3. Обширная поддержка кодировок и автообнаружений для работу с иностранными нестандартными языками\n",
    "   4. Модульность (через сигналы и грамотно описанное АПИ)(middlewares, extensions, and pipelines).\n",
    "   5. Встроенные расширения и плагины для:\n",
    "      1. cookies and session handling\n",
    "      2. HTTP features like compression, authentication, caching\n",
    "      3. user-agent spoofing\n",
    "      4. robots.txt\n",
    "      5. crawl depth restriction\n",
    "3. Интеграция с Selenium через Scrapy-Selenium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А, ну и пример\n",
    "\n",
    "Сложнейший выбор сделают лишь сильнейшие. Поэтому мы сделаем самый маскулинный и dominance asserting мув который можно представить - запарсим Евраз.Маркет и по пути парсинга расскажу какие приколы вообще лежат в Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему Евраз.Маркет? Потому что изначально я хотел запарсить Северсталь.Маркет? А потому что придется поиграться с селениумом а мне хочется поиграться с селениумом (у челов слишком много уровней защиты), и мы будем использовать не просто Scrapy, а Scrapy-Selenium. Почему я не стал парсить Северсталь.Маркет? Потому что у северстали слишком сильная защита от ботов, для нее нужно использовать всю силу Selenium. Scrapy не позволяет этого сделать (потому что он выбрасывает свой драйвер, а не тот, что нам нужен), а взрывать исходный код библиотеки, чтобы селениум нельзя было задетектить не хочется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь уже само дело:\n",
    "\n",
    "Код, к сожалению, сложно интегрировать в ноутбук, потому что под Scrapy-парсер требуется целый проект, поэтому описание будет в виде \n",
    "```python\n",
    "# имя файла\n",
    "<код>\n",
    "```\n",
    "\n",
    "[ссылка на сам проект](https://github.com/wolfrahmmetall/scrapy-report)\n",
    "\n",
    "Корневой (~) папкой считаем саму основную папку `report`, в которой содержится `report`, в которой содержится еще один `report` (Scrapy это нужно)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ## Интеграция Scrapy-selenium\n",
    "   Прописываем в терминале:\n",
    "    ```sh\n",
    "    report $ scrapy startproject report\n",
    "    ```\n",
    "    scrapy создаст нам новый проект:\n",
    "    ```\n",
    "    report\n",
    "    │   ├── __init__.py\n",
    "    │   ├── __pycache__\n",
    "    │   │   ├── __init__.cpython-311.pyc\n",
    "    │   │   └── settings.cpython-311.pyc\n",
    "    │   ├── items.py\n",
    "    │   ├── middlewares.py\n",
    "    │   ├── pipelines.py\n",
    "    │   ├── settings.py\n",
    "    │   └── spiders\n",
    "    └── scrapy.cfg\n",
    "    ```\n",
    "    В котором уже будут сгенерированы все нужные файлы\n",
    "\n",
    "    Также в корневую директорию добавим Chromedriver  \n",
    "    Зачем? Затем, что мы работаем на Chrome (Version 130.0.6723.117, arm64) (Firefox круче, но проверить кто-то должен) \n",
    "\n",
    "    Также есть проблема с тем, что Scrapy-selenium нативно не поддерживает `Selemium4`. Но есть обход: https://github.com/clemfromspace/scrapy-selenium/blob/5c3fe7b43ab336349ef5fdafe39fc87f6a8a8c34/scrapy_selenium/middlewares.py  \n",
    "    засовываем его в `middlewares.py` по `Location` из `python -m pip show scrapy-selenium`\n",
    "\n",
    "    Далее заходим в `settings.py` нашего проекта\n",
    "    и прописываем (заменяем) следующее:\n",
    "    ```python\n",
    "    DOWNLOADER_MIDDLEWARES = {\n",
    "   \"scrapy_selenium.SeleniumMiddleware\": 800\n",
    "   }\n",
    "   DEFAULT_REQUEST_HEADERS = {\n",
    "   \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "   \"Accept-Language\": \"ru\",\n",
    "   \"User-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36\" # Северсталь банит всех роботов, работают только ЮА\n",
    "   }\n",
    "\n",
    "   SELENIUM_DRIVER_ARGUMENTS = [\"--headless=new\"] \n",
    "    ```\n",
    "\n",
    "   и о счастье, все должно работать\n",
    "\n",
    "2. ## Пауки\n",
    "   Самая главная фича Scrapy -- по сути парсер бот, который способен через движок Scrapy самостоятельно отправлять запросы и парсить сайты\n",
    "\n",
    "   Scrapy умеет сам их генерировать с помощью шаблонов с помощью команды в терминале:\n",
    "   ```sh\n",
    "   report $ scrapy genspider spider name url\n",
    "   ```\n",
    "   он создаст папку `spiders` и в ней паука \n",
    "\n",
    "   нам нужен: \n",
    "   ```sh\n",
    "   report $ scrapy genspider armatura https://market.severstal.com/ru/ru/o/c/46/96\n",
    "   ```\n",
    "\n",
    "   и Scrapy нам создал\n",
    "   ```python\n",
    "   #report/report/spiders/armatura.py\n",
    "   import scrapy\n",
    "\n",
    "   class ArmaturaSpider(scrapy.Spider):\n",
    "      name = \"armatura\"\n",
    "      allowed_domains = [\"market.severstal.com\"]\n",
    "      start_urls = [\"https://market.severstal.com/ru/ru/o/c/46/96\"]\n",
    "\n",
    "      def parse(self, response):\n",
    "         pass\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. мы парсим с селениумом, то `parse` выглядит так:\n",
    "\n",
    "```python\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = \"https://evraz.market/metalloprokat/armatura/armatura_riflenaya/\"\n",
    "        yield SeleniumRequest(url=url, callback=self.parse)\n",
    "        url = \"https://evraz.market/metalloprokat/armatura/armatura_riflenaya/PAGEN_4=2\"\n",
    "        yield SeleniumRequest(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response: scrapy.http.Response):\n",
    "        driver: Chrome = response.request.meta['driver']\n",
    "        products = driver.find_element(By.CSS_SELECTOR, \"tbody\")\n",
    "        for product in products.find_elements(By.CSS_SELECTOR, '.catalog-item'):\n",
    "            arm_name = product.find_element(By.CSS_SELECTOR, 'span.underline-stroke-link').find_element(By.CSS_SELECTOR, 'span').text\n",
    "            length = product.find_element(By.CSS_SELECTOR, '.main-info__question').text.split(' ')[-1]\n",
    "            left = product.find_element(By.CSS_SELECTOR, '.catalog-page-stock-status-block').text\n",
    "            price = [prod.text for prod in product.find_elements(By.CSS_SELECTOR, '.text-nowrap')[:2]]\n",
    "            yield {\n",
    "                'name': arm_name,\n",
    "                'length': length,\n",
    "                'remaining': left,\n",
    "                'price range': price\n",
    "            }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тогда, прогнав в терминале\n",
    "```sh\n",
    "~$ scrapy crawl <имя паука> *keys\n",
    "```\n",
    "можно запустить скрэпи движок и заставить бот работать\n",
    "\n",
    "Можно прописать `-o` и вывести все данные из паука в `csv` или `json` файл и `-L` для установки выводимого в терминал уровня логгирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Как это работает?\n",
    "При запуске движка Scrapy сначала прогоняет свой метод `start_requests`, `yield`'я реквесты по ссылкам. `start_request` может как просто использовать ссылки из поля `start_urls`, так и может быть переопределен.  \n",
    "Запросы в `start_requests` могут быть как просто `scrapy.http.Request`, так и `SeleniumRequest` из `Scrapy-selenium`, который я использовал. Суть одна, ему нужен callback -- функция обработчик, который возвращает данные в удобном формате, чтобы их можно было передать в json или csv файл. Почему `yield`? Потому что движок асинхронный.\n",
    "\n",
    "При этом нужно обратить внимание на одну вещь -- в `parse` передается аргумент `response: scrapy.http.Response`. Это очень важно, потому что в нем содержится вся информация о сайте, в частности html-разметка, а значит ее можно передать в `webdriver` и спарсить"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='ff0000;'> Но это же не все пауки</font>\n",
    "\n",
    "Scrapy дает еще несколько шаблонов пауков, но они не очень полезны в примере, поэтому приведу примеры с документации:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='ffff00'> Ползущий паук</font>\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "\n",
    "\n",
    "class MySpider(CrawlSpider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\"http://www.example.com\"]\n",
    "\n",
    "    rules = (\n",
    "        # Extract links matching 'category.php' (but not matching 'subsection.php')\n",
    "        # and follow links from them (since no callback means follow=True by default).\n",
    "        Rule(LinkExtractor(allow=(r\"category\\.php\",), deny=(r\"subsection\\.php\",))),\n",
    "        # Extract links matching 'item.php' and parse them with the spider's method parse_item\n",
    "        Rule(LinkExtractor(allow=(r\"item\\.php\",)), callback=\"parse_item\"),\n",
    "    )\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        self.logger.info(\"Hi, this is an item page! %s\", response.url)\n",
    "        item = scrapy.Item()\n",
    "        item[\"id\"] = response.xpath('//td[@id=\"item_id\"]/text()').re(r\"ID: (\\d+)\")\n",
    "        item[\"name\"] = response.xpath('//td[@id=\"item_name\"]/text()').get()\n",
    "        item[\"description\"] = response.xpath(\n",
    "            '//td[@id=\"item_description\"]/text()'\n",
    "        ).get()\n",
    "        item[\"link_text\"] = response.meta[\"link_text\"]\n",
    "        url = response.xpath('//td[@id=\"additional_data\"]/@href').get()\n",
    "        return response.follow(\n",
    "            url, self.parse_additional_page, cb_kwargs=dict(item=item)\n",
    "        )\n",
    "\n",
    "    def parse_additional_page(self, response, item):\n",
    "        item[\"additional_data\"] = response.xpath(\n",
    "            '//p[@id=\"additional_data\"]/text()'\n",
    "        ).get()\n",
    "        return item\n",
    "```\n",
    "\n",
    "Зачем он нужен? Он умеет переходить по ссылкам по определенным *правилам* -- то есть потенциально мы можем рекурсивно запарсить весь сайт с одной страницы.  \n",
    "При этом сами правила -- callback:  \n",
    "```python\n",
    "class scrapy.spiders.Rule(link_extractor: LinkExtractor | None = None, callback: CallbackT | str | None = None, cb_kwargs: dict[str, Any] | None = None, follow: bool | None = None, process_links: ProcessLinksT | str | None = None, process_request: ProcessRequestT | str | None = None, errback: Callable[[Failure], Any] | str | None = None)[source]\n",
    "```\n",
    "\n",
    "Как видим, мы используем несколько `callback`ов для обработки страниц по ссылкам, по сути, аналогов `parse` и так постепенно спарсим все дерево"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='ffff00'>XMLFeedSpider и CSVFeedSpider</font>\n",
    "\n",
    "```python\n",
    "from scrapy.spiders import XMLFeedSpider\n",
    "from myproject.items import TestItem\n",
    "\n",
    "\n",
    "class MySpider(XMLFeedSpider):\n",
    "    name = \"example.com\"\n",
    "    allowed_domains = [\"example.com\"]\n",
    "    start_urls = [\"http://www.example.com/feed.xml\"]\n",
    "    iterator = \"iternodes\"  # This is actually unnecessary, since it's the default value\n",
    "    itertag = \"item\"\n",
    "\n",
    "    def parse_node(self, response, node):\n",
    "        self.logger.info(\n",
    "            \"Hi, this is a <%s> node!: %s\", self.itertag, \"\".join(node.getall())\n",
    "        )\n",
    "\n",
    "        item = TestItem()\n",
    "        item[\"id\"] = node.xpath(\"@id\").get()\n",
    "        item[\"name\"] = node.xpath(\"name\").get()\n",
    "        item[\"description\"] = node.xpath(\"description\").get()\n",
    "        return item\n",
    "```\n",
    "\n",
    "Удобный паук, чтобы парсить `XML` и `CSV` файлы, он говорит обо всем за себя"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Только пауки?\n",
    "\n",
    "Тоже нет. Есть класс `Item` и его наследники. Которые позволяют создать наш возвращаемый тип вида json-словаря с помощью полей типа `Field()` (честно? юзлесс)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А почему именно Scrapy?\n",
    "Валидный вопрос. С Selenium можно спарсить страшные вещи вроде Северсталь.Маркета. А для простых сайтов хватает и просто requests. НО Scrapy обладает гораздо более тонкой настройкой.\n",
    "Можно установить Скачивающих посредников, аргументы для селениума и много чего другого. К тому же не надо возиться с драйверами в селениумом -- тут все уже готово"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В репозитории можно посмотреть на примеры парсеров для разных страниц и баш скрипт, который запускает их вместе. Там и многостраничный парсинг (отладить я его не успел, потому что превысил лимит запросов, даже с рандомными юзер агентами), и уровни логгировния, и многое прочее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рандомные юзер агенты? А что это\n",
    "Юзер Агент --- программа, способная имитировать человека при использовании сайтов. Но проблема в том, что сайт способен обнаружить на каком IP+данных юзер агента бот находится, поэтому банит его.\n",
    "Но в то же время, Scrapy с помощью `scrapy_user_agents` (тоже модуль, скачивается из пипа) предоставляет возможность рандомно подставлять рандомного юзер агента. Соответственно, сайту будет сложнее обнаружить нас"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
